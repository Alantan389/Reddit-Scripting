{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit has free URI, HERE ARE THE STEPS TO GET URL FROM REDDIT\n",
    "\n",
    "client_id\n",
    "client_secret\n",
    "redirect_uri\n",
    "user_agent\n",
    "\n",
    "Go to https://www.reddit.com/prefs/apps and log in to your Reddit account.\n",
    "\n",
    "Click on the \"are you a developer ? Create App\" button.\n",
    "\n",
    "Select the type of app you want to create. For web scraping, select \"Script.\"\n",
    "\n",
    "Give your app a name, description, and redirect URI. The redirect URI can be any valid URI, such as \"http://localhost:8000.\" This field is required, but not used for script-type apps.\n",
    "\n",
    "Click the \"Create App\" button.\n",
    "\n",
    "Once your app is created, you should see your client_id and client_secret on the app details page.\n",
    "\n",
    "To get your user_agent, simply enter any string value you want for the user_agent field. This value should describe your app and be unique to your app. For example, you could use something like \"my-reddit-scraper-bot/1.0\" as your user_agent.\n",
    "\n",
    "Once you have your client_id, client_secret, and user_agent, you can use them to authenticate your connection to the Reddit API in your Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw \n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(client_id='xxx',\n",
    "                     client_secret='xxx',\n",
    "                     redirect_uri=\"xxx\",\n",
    "                     user_agent='xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('datascience')\n",
    "search_results = subreddit.search('jobs', limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What skills/jobs makes the most money in Data Science/Data Analysis?\n",
      "The pareto principle roughly states that you get 80% of value from 20% of the work. What jobs or skills in the data world get you the most income or monetary value?\n",
      "Everyone here seems focused on advanced modelling and CS skills. If you want a high paying job, IMO just focus on SQL and business metrics\n",
      "Iâ€™ve been browsing this sub for over 5 years. Back when I started, I had a business undergrad degree and wanted to break into the world of advanced ML/AI. \n",
      "\n",
      "What Iâ€™ve found since getting into big tech as a # Data Scientist (if you listen to music odds are you use my companyâ€™s product) is that 99.99% of the skills involved are: \n",
      "\n",
      "- SQL\n",
      "\n",
      "- grade school math/algebra \n",
      "\n",
      "- common sense to apply SQL results to business questions \n",
      "\n",
      "- presentation/communication skills \n",
      "\n",
      "My current TC is 300k (205k base) and I have yet to need anything fancier than a bar graph or a line chart. Yes thereâ€™s teams internally that use much more advanced math/causal inference, but for the majority of folks browsing here and looking for a career path, youâ€™re over complicating things. \n",
      "\n",
      "I wonder why so many online look down at pseudo data analyst roles when they can have just as big of an impact as your cutting edge ML folks internally by convincing the CEO to cut/launch a product, and the pay is more than enough to live off of.\n",
      "\n",
      "Iâ€™m on the interview panel for senior/staff DS and frequently get PHDs from top schools who donâ€™t pass the bar because they canâ€™t answer basic business case questions and canâ€™t write basic SQL. They want to apply ML to the answer of every question when itâ€™s not necessary.\n",
      "Having trouble finding a job. Ex-FAANG.\n",
      "I am an ex-FAANG data scientist. I didn't get laid off, but I went back to school to get an MBA during the last 2 years and am graduating in May during a supposed \"recession\". Tons of recent FAANG layoffs and hiring freezes have made me gun shy to apply to FAANG again. However, it appears that companies all over the world are actually having hiring freezes and layoffs, not just FAANG and Big Tech. I've been sending out resume's nonstop and I am starting to feel like I am wasting my time. I don't even get rejection emails. Just completely ghosted. No responses from 300+ applications sent in the past month or two. Literally ZERO interviews or phone screens by HR. Literally ZERO emails back asking about my availability. It's like a ghost town. I have a degree in CS from a top 50 university, an MS in Data Science from a top 30 school, and a soon to be MBA from an Ivy League school. I am trying to break into upper management in the Data Analytics domain. I am in a weird place because I feel like I can command FAANG level salaries, but FAANG isn't hiring anymore. Also, I feel like other companies see that I have FAANG and Ivy League MBA on my resume and think I want too much money and will go back to FAANG once the economy improves and hiring picks up again. I am basically screwed.\n",
      "300,000+ Tech jobs have been vanished in the last 12 months. (Sad but true fact)\n",
      "\n",
      "New Job Didnâ€™t Match the Description: what to do?\n",
      "Started a new job last week that has ended up being more of an IT job than a data job, doing some light analytics in Splunk. Did not match description or expectations in interview. \n",
      "\n",
      "This job is fully remote and pays $130K. Previous job was on site and paid $90K.  \n",
      "\n",
      "What should I do? Stick it out for 6 months and leave? Begin interviewing ASAP? Another option is just quitting and doing my MS in DS full time. WWYD?\n",
      "Is the entire job market down right now or is it just data science?\n",
      "\n",
      "How bad is the job market right now?\n",
      "I've been feeling burnt out for a while now and want to take a short career break before looking for a new job. Is the job market as bad right now as people are making it out to be? \n",
      "\n",
      "For context I'm European and have a master's degree and 2.5 years work experience.\n",
      "Why do Data Scientists not think AI will take their jobs when developers, SWE, and other tech workers worry about it?\n",
      "This isn't the typical AI question so please don't tell me to delete it. AI stuff is all over like everyone knows already and people wonder what the heck is gonna happen with work in general across many white collar sectors. I genuinely don't know how this will play out but I don't think it's unwarranted to feel even Data Science will be hit and is not immune to it. I see many developers and many with considerable experience and expertise still concerned. I think people overly hairsplit and get into polarized arguments while missing this central point. AI will never be consciousness itself but that doesn't matter because it can still wipe out our jobs and do it them better and faster. This may not be too far off in the future. What you guys feel makes you different and not as concerned as other tech workers about being impacted by AI?\n",
      "New job, company has not prioritized data and need some advice\n",
      "Hey data folks, I'm a new hire in charge of data at a small financial software startup and need your opinion on how to go about tackling this. I joined a few weeks ago and have been put in charge of coming up with a data catalog, but right now, all their data is either on spreadsheets or just on our CRM. We have a solid budget, but I'd love some advice on what can work quickly, as I'd like to start actually taking advantage of the data and get ahead of the problem. What are the most useful products out there?\n",
      "Are \"easy apply\" LinkedIn jobs even worth it?\n",
      "I see them all the time, they're usually amazing in terms of compensation, and usually 200+ candidates within a few days. Of the ones that I applied to, I never even hear back. I find it incredibly unlikely that I'll stand out, and it makes me wonder if it's even worth submitting an ad-hoc resume highlighting my relevant expertise if I'm not going to hear back. \n",
      "\n",
      "What do you all do? Take time to tailor the resume for each position? Submit a generic resume that kinda fits the description, in hopes that it'll get you through the door? Not even bother? \n",
      "\n",
      "Sorry if the question is silly, I'm just a little peeved with the whole thing.\n"
     ]
    }
   ],
   "source": [
    "for result in search_results:\n",
    "    print(result.title)\n",
    "    print(result.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gh1dj9\n",
      "kuc6tz\n",
      "g7nfvb\n",
      "lui92h\n",
      "ohxnts\n",
      "n2f0ld\n",
      "k8nyf8\n",
      "vkxsf2\n",
      "oeg6nl\n",
      "hiv3vf\n",
      "m554cq\n",
      "hohvgq\n",
      "xdv6nz\n",
      "j0oyk6\n",
      "tj3kek\n",
      "fg73za\n",
      "leq2kf\n",
      "oisl3e\n",
      "xit874\n",
      "klbvaw\n",
      "wp2vqk\n",
      "e6iy5o\n",
      "ybnnra\n",
      "dudedh\n",
      "gc2wo9\n",
      "zhrgln\n",
      "peremu\n",
      "pmqtj9\n",
      "juv419\n",
      "hciw10\n",
      "10ys3md\n",
      "ljftgi\n",
      "s4tu5x\n",
      "11sboh1\n",
      "uqzrul\n",
      "ejvao9\n",
      "xtxe6f\n",
      "dh2xfs\n",
      "jm86z9\n",
      "ma8xbq\n",
      "frkgr7\n",
      "qzluvi\n",
      "10mmm38\n",
      "vwlmoo\n",
      "p0moqj\n",
      "748cco\n",
      "tw7kc0\n",
      "o843t5\n",
      "g7wvpb\n",
      "qrjmge\n",
      "xcdnd8\n",
      "eb240z\n",
      "i1aafb\n",
      "kythnj\n",
      "tjfxtx\n",
      "kp5pxi\n",
      "f6xk72\n",
      "rozxuk\n",
      "sfbtds\n",
      "mouyp0\n",
      "zw9mtn\n",
      "wmypmh\n",
      "p59a8u\n",
      "5z8110\n",
      "jcuch4\n",
      "uw2a27\n",
      "yik3k5\n",
      "xyxe8w\n",
      "rdsepx\n",
      "g8v44c\n",
      "t7qe6b\n",
      "o3804y\n",
      "qo4kp8\n",
      "fvu3qu\n",
      "g4jc29\n",
      "jybogw\n",
      "ah0q69\n",
      "ia2aob\n",
      "10tovhn\n",
      "m47an8\n",
      "tq93vt\n",
      "ja54n9\n",
      "eiiv4u\n",
      "10ch0kw\n",
      "mmzbgq\n",
      "vbcfpg\n",
      "thsx8t\n",
      "zo5bwf\n",
      "yaqlvi\n",
      "j0btow\n",
      "uks8zr\n",
      "w2282t\n",
      "ro2567\n",
      "da5mhe\n",
      "cqffii\n",
      "g6og9l\n",
      "umse6v\n",
      "ijkkbb\n",
      "orybjg\n",
      "p6hsoh\n",
      "w6kj9y\n",
      "xtd8kc\n",
      "v5f8et\n",
      "8n04hp\n",
      "10y2rrx\n",
      "uls349\n",
      "68y8bb\n",
      "ugg2bz\n",
      "ey8icu\n",
      "cb0gte\n",
      "e9cdf3\n",
      "ylfpqx\n",
      "qbnf3s\n",
      "ggspu2\n",
      "uyratt\n",
      "6l2esd\n",
      "tq5i58\n",
      "q9hhqt\n",
      "d74usq\n",
      "lozys9\n",
      "d6buto\n",
      "kfip3w\n",
      "10nyhcl\n",
      "s0dn5b\n",
      "cu26yc\n",
      "tn3xh2\n",
      "106q6m9\n",
      "qph4tx\n",
      "tag8l5\n",
      "xbl58o\n",
      "ztwkky\n",
      "h98tt5\n",
      "glfdmm\n",
      "g8s1af\n",
      "hlkwm1\n",
      "ur5521\n",
      "p6lpws\n",
      "129sqba\n",
      "zqsseu\n",
      "x6ji1j\n",
      "124cshz\n",
      "wjycg3\n",
      "xh6voz\n",
      "mcy1zw\n",
      "f20n3x\n",
      "92x6ll\n",
      "qdai89\n",
      "ggakn3\n",
      "xbj6cn\n",
      "uqk878\n",
      "qypj5f\n",
      "u6dlyr\n",
      "10h4zfl\n",
      "j4auif\n",
      "wiqjxv\n",
      "oikye2\n",
      "d7ad2y\n",
      "uz12cu\n",
      "jdeyp9\n",
      "rmcgwt\n",
      "k3ygrc\n",
      "tqbez2\n",
      "8l5w56\n",
      "11rizyb\n",
      "wz68mz\n",
      "qjpcut\n",
      "8kbmyn\n",
      "4w6tsv\n",
      "ycgmbu\n",
      "p7hpd9\n",
      "f1rufm\n",
      "7y6g79\n",
      "vwvg8j\n",
      "eesoav\n",
      "10ujsk5\n",
      "vljjur\n",
      "u0o0yy\n",
      "zo2nl1\n",
      "e0puay\n",
      "mp6ink\n",
      "7vuqvc\n",
      "gydxzd\n",
      "yhrlpj\n",
      "qeihw2\n",
      "yzap5b\n",
      "o468ms\n",
      "rjg6ng\n",
      "11d4uys\n",
      "ia93ao\n",
      "nnqjjc\n",
      "11vozd5\n",
      "ohk6b7\n",
      "i5yres\n",
      "qgx1vm\n",
      "pymzvn\n",
      "vi5cvr\n",
      "8p169l\n",
      "ex2sks\n",
      "if1sdg\n",
      "gpmbpl\n",
      "ajgzoc\n",
      "xat19z\n",
      "sroth8\n",
      "10nxqfg\n",
      "bvzc7w\n",
      "dv7mdc\n",
      "ymo07f\n",
      "wbwkwb\n",
      "dijadz\n",
      "n3b1m6\n",
      "e1r0ou\n",
      "11uzhqa\n",
      "7fro3g\n",
      "7gls3j\n",
      "zowhlo\n",
      "yxzaz3\n",
      "ypr93q\n",
      "eufeqm\n",
      "fni5ow\n",
      "11ddeft\n",
      "xgnt6k\n",
      "w759hp\n",
      "vapbkh\n",
      "xslpwt\n",
      "xf6ifb\n",
      "10ikd4i\n",
      "8hdby5\n",
      "10pkvru\n",
      "fvwwzj\n",
      "w4k8w8\n",
      "q97fpv\n",
      "8rdpwy\n",
      "hudog1\n",
      "nue01q\n",
      "kibblu\n",
      "x5dwm5\n",
      "gyv6to\n",
      "wz7pfk\n",
      "7b7ghl\n",
      "121t6tp\n",
      "ynx8o8\n",
      "6se5zj\n",
      "vx6kcx\n",
      "yh3gmq\n",
      "xwv9m3\n",
      "ff81cd\n",
      "cmit48\n",
      "rgb80b\n",
      "10de0j4\n",
      "r3c970\n",
      "tqe3y6\n",
      "at80o8\n",
      "ktnwcv\n",
      "6z51xb\n",
      "uzt23p\n",
      "roufb5\n",
      "1200b4s\n",
      "a88ejl\n",
      "kbnlte\n",
      "b2q0nd\n",
      "g12zmd\n",
      "wp96s5\n",
      "w1ybgk\n",
      "ra6teb\n",
      "zvbjot\n",
      "s0kndc\n",
      "c7l6fo\n",
      "xfnjqa\n",
      "fm17ja\n",
      "xd9ewk\n",
      "xhahv5\n",
      "npurud\n",
      "lkn4rl\n",
      "uiuqwp\n",
      "bbprie\n",
      "mocpgj\n",
      "wcalkv\n",
      "ael5rz\n",
      "ssfijc\n",
      "k7iytr\n",
      "bh3kko\n",
      "1032pgs\n",
      "ab4207\n",
      "ngn6at\n",
      "hzdiru\n",
      "xnbv8e\n",
      "opnzmc\n",
      "y89xqw\n",
      "lcuq4b\n",
      "sh4otq\n",
      "wvsu4r\n",
      "hpajb2\n",
      "w5w0jq\n",
      "nmyg3i\n",
      "wi2gil\n",
      "lgiug8\n",
      "efwlcs\n",
      "8qh7e5\n",
      "u1ivbw\n",
      "124eyso\n",
      "di2fez\n",
      "bmmyae\n",
      "hgnlf5\n",
      "w0pxwh\n",
      "aau4jv\n",
      "sab6tk\n",
      "8midpw\n",
      "a6cbzm\n",
      "vye69k\n",
      "hu006c\n",
      "mxg7pv\n",
      "ky27rn\n",
      "heiyqq\n",
      "f981hm\n",
      "kr63ot\n",
      "qeo7fx\n",
      "pk613b\n",
      "110s8ui\n",
      "bn6phx\n",
      "r5thf9\n",
      "wt6ztg\n",
      "pqpl7m\n",
      "cmhctd\n",
      "8p9car\n",
      "uryrot\n",
      "f5d3nk\n",
      "8uibp4\n",
      "iorbjg\n",
      "m3in2j\n",
      "11mlwty\n",
      "qo1l35\n",
      "smbj1o\n",
      "pc2g4c\n",
      "11u1xb7\n",
      "t78zoq\n",
      "rsfdlx\n",
      "cb9wie\n",
      "10nodn4\n",
      "fkgfax\n",
      "e03azf\n",
      "v42pej\n",
      "rxm4ej\n",
      "vm9xjz\n",
      "gfnax4\n",
      "uk62j3\n",
      "70vuj5\n",
      "11nwxd6\n",
      "qymvys\n",
      "klppo6\n",
      "p29bae\n",
      "9t9kz4\n",
      "7ts8my\n",
      "10zvb04\n",
      "wn61bp\n",
      "jhx3cv\n",
      "krkxog\n",
      "10zmz2d\n",
      "cgwvds\n",
      "qj3uhj\n",
      "om7kq3\n",
      "hkiyir\n",
      "qt2tws\n",
      "gj475j\n",
      "t6lcyz\n",
      "lqrek7\n",
      "dhe767\n",
      "gtaq94\n",
      "eyg2hv\n",
      "8i3zll\n",
      "vxuxhv\n",
      "fdw0ax\n",
      "xgu9wg\n",
      "rh22z9\n",
      "ohsz93\n",
      "jvq4jw\n",
      "j1q5w2\n",
      "11ybjsi\n",
      "11jgig0\n",
      "ujurw5\n",
      "aohn8w\n",
      "uf552a\n",
      "myiw7e\n",
      "drde9q\n",
      "bupmyf\n",
      "w4jg7q\n",
      "n10o03\n",
      "ge1vxi\n",
      "xix8ef\n",
      "qxzwse\n",
      "s3mjqf\n",
      "jh9wej\n",
      "vzmcc2\n",
      "efpjcp\n",
      "oss2e3\n",
      "lqozmp\n",
      "128lo83\n",
      "vceaxx\n",
      "y6w5ab\n",
      "li2afr\n",
      "ons0gh\n",
      "m73sy7\n",
      "k09wu5\n",
      "11izjc1\n",
      "fnh8zm\n",
      "oq33wd\n",
      "4v58b2\n",
      "4f07rp\n",
      "i4w86p\n",
      "beoxx8\n",
      "11un32i\n",
      "xmpv89\n",
      "dgroou\n",
      "ap4gzx\n",
      "frgoje\n",
      "8vbkti\n",
      "excxlv\n",
      "c4ylga\n",
      "zubg2u\n",
      "uqo085\n",
      "5y61bg\n",
      "zys7g5\n",
      "donbz7\n",
      "mimpre\n",
      "ldvl72\n",
      "ejbwvb\n",
      "kw9xk7\n",
      "kagp2b\n",
      "73n9pm\n",
      "9f18t6\n",
      "ps1ysk\n",
      "m3boyo\n",
      "kg2g11\n",
      "vbh2vx\n",
      "v6sv06\n",
      "s9zcyq\n",
      "mxxnki\n",
      "11hscl1\n",
      "jig7pv\n",
      "d2rym1\n",
      "zpraee\n",
      "y7708w\n",
      "jlef67\n",
      "jtbr8c\n",
      "99qkrk\n",
      "oyhnzj\n",
      "7ly5gi\n",
      "tt3in6\n",
      "s0uhca\n",
      "oek26v\n",
      "ia8gc6\n",
      "k2pd9n\n",
      "i0l5m9\n",
      "j4avac\n",
      "qsw47b\n",
      "ebdhi6\n",
      "10jd28b\n",
      "ac6wsd\n",
      "7c408f\n",
      "r79r9y\n",
      "jboe91\n",
      "aoacek\n",
      "11w03sy\n",
      "us2a9j\n",
      "ssqt3h\n",
      "rgykys\n",
      "zfrynz\n",
      "sivgoj\n",
      "v0wjy2\n",
      "jvwgq3\n",
      "9vihdt\n",
      "kvs1ex\n",
      "6h6ao0\n",
      "refiro\n",
      "iwl0b9\n",
      "e5qx5d\n",
      "3s4qpm\n",
      "j6gbxm\n",
      "n3v93k\n",
      "ci358m\n",
      "iwo0d4\n",
      "frd031\n",
      "713hnw\n",
      "d5mn4l\n",
      "osapch\n",
      "11rc02e\n",
      "g61p08\n",
      "127wy7i\n",
      "qwqbxn\n",
      "11uk8ti\n",
      "f29l4v\n",
      "p19yur\n",
      "89nzbm\n",
      "5b5ej8\n",
      "nxyr25\n",
      "mbhewa\n",
      "hipozj\n",
      "dlg1rf\n",
      "kx0ies\n",
      "128s80d\n",
      "10bkjdk\n",
      "j4jrln\n",
      "qlilnf\n",
      "77m2k2\n",
      "ntn1eg\n",
      "33n77s\n",
      "ncdy6m\n",
      "gv3i57\n",
      "i8dw0j\n",
      "p41hko\n",
      "najnjg\n",
      "8hz8xy\n",
      "b63l98\n",
      "s5grvj\n",
      "ltjyr5\n",
      "sup40t\n",
      "ntiv0z\n",
      "bo8a0c\n",
      "wla15x\n",
      "fo5stq\n",
      "xch39o\n",
      "ulvdgm\n",
      "8vgk3u\n",
      "ohche1\n",
      "lvfua3\n",
      "qto1kr\n",
      "mpe7le\n",
      "ek5zwv\n",
      "m71ijk\n",
      "pdwxxz\n",
      "7s1n5j\n",
      "ltkt9s\n",
      "gonna8\n",
      "869ml6\n",
      "1169uzy\n",
      "lvwt3l\n",
      "i3o4fe\n",
      "pizllt\n",
      "4casci\n",
      "11mzqxu\n",
      "w2via6\n",
      "ql4e39\n",
      "jpljoo\n",
      "ecchg8\n",
      "hhfqbl\n",
      "kod9ze\n",
      "7jphff\n",
      "10w6g7n\n",
      "nmaguz\n",
      "irauuz\n",
      "a3gm3u\n",
      "zfeh67\n",
      "r6tfsb\n",
      "l0kz3n\n",
      "83ohd5\n",
      "kqazpd\n",
      "dkcspv\n",
      "hecc72\n",
      "p2wfp2\n",
      "gz3oc4\n",
      "vwcp5o\n",
      "e9apif\n",
      "x7hh75\n",
      "gf9hrs\n",
      "ipstlf\n",
      "ufvlfm\n",
      "le2co0\n",
      "9zl84o\n",
      "sm96f5\n",
      "igwruq\n",
      "syjt0c\n",
      "81050c\n",
      "hwiams\n",
      "d8nlqf\n",
      "b3alkh\n",
      "uo589a\n",
      "aqwcyx\n",
      "8nl2ps\n",
      "123b66w\n",
      "11l5mg2\n",
      "cok47z\n",
      "pigtg9\n",
      "mdldtt\n",
      "5ysono\n",
      "h96nz8\n",
      "seufwd\n",
      "ha6laa\n",
      "gkw681\n",
      "zht9og\n",
      "izh8a7\n",
      "52k3hp\n",
      "wi05tg\n",
      "reh9cv\n",
      "hmqhpy\n",
      "7mlwf4\n",
      "obw2xc\n",
      "o04ort\n",
      "6go2n9\n",
      "am1yeq\n",
      "cp51po\n",
      "7if6h1\n",
      "6sndko\n",
      "wec5hs\n",
      "qhu09k\n",
      "h8qhsg\n",
      "er3ng8\n",
      "edgai0\n",
      "3a1ebc\n",
      "jm0lhu\n",
      "hpv0wm\n",
      "dnmlyz\n",
      "10kqhyh\n",
      "11wxabh\n",
      "tivnnb\n",
      "101t0vt\n",
      "ij9gxu\n",
      "fpi8qf\n",
      "f8b38r\n",
      "e8ihnp\n",
      "7htg5f\n",
      "11awp4n\n",
      "oaambv\n",
      "lbr6qi\n",
      "r1wcjt\n",
      "lpf8m6\n",
      "kzr4mg\n",
      "bjl6r0\n",
      "zgrkkr\n",
      "yp082p\n",
      "cw39dx\n",
      "a6lq4e\n",
      "a0xfc2\n",
      "q9phnq\n",
      "pe9a2j\n",
      "p8rcm3\n",
      "okscnp\n",
      "nkbqx6\n",
      "mtev6w\n",
      "8kifb0\n",
      "kps6fl\n",
      "s01us1\n",
      "10nccbg\n",
      "jg3vbh\n",
      "ny86g7\n",
      "budoyb\n",
      "oyi7a1\n",
      "lch48m\n",
      "vtzw0b\n",
      "8er6c3\n",
      "12brxc1\n",
      "u5rnss\n",
      "wtbt9d\n",
      "rmue6j\n",
      "ikbbsb\n",
      "8t0l40\n",
      "fh2rr6\n",
      "tx132u\n",
      "120usfk\n",
      "xgijzo\n",
      "gqns9k\n",
      "h7dtrq\n",
      "49n2e5\n",
      "ov3itd\n",
      "lhhe8e\n",
      "hte2kb\n",
      "dn6xrr\n",
      "fuyoai\n",
      "8tq81f\n",
      "8psghc\n",
      "7780ok\n",
      "120qzg1\n",
      "jzol5g\n",
      "p9aisc\n",
      "bb9umg\n",
      "od2csk\n",
      "ed2pve\n",
      "a8ku09\n",
      "squ4oq\n",
      "mm7w8r\n",
      "dk9eq3\n",
      "djju8a\n",
      "6dg8ed\n",
      "vl7iut\n",
      "n62qhn\n",
      "iyxz7o\n",
      "imwl0z\n",
      "ztbsf5\n",
      "pf9j9s\n",
      "l417cm\n",
      "hrawam\n",
      "apwm0q\n",
      "xqmj9q\n",
      "t14ju7\n",
      "lej57x\n",
      "jpznqe\n",
      "12ay0vt\n",
      "11fbccz\n",
      "yy11d8\n",
      "iyxij1\n",
      "gjq1c1\n",
      "upfigh\n",
      "i1sp9q\n",
      "xl9zc1\n",
      "fhsxau\n",
      "ub045v\n",
      "t45n67\n",
      "o1mxqp\n",
      "ljkmr7\n",
      "ew8oxq\n",
      "gfq9kp\n",
      "s6spou\n",
      "fkg06u\n",
      "8mgs8k\n",
      "xvje2n\n",
      "l0l0oc\n",
      "dsr6j5\n",
      "6t58ks\n",
      "40kh35\n",
      "yik5ze\n",
      "sbnq4f\n",
      "dqf09j\n",
      "wrqd26\n",
      "z60wuh\n",
      "rm6f7i\n",
      "b9iyi6\n",
      "dv5axp\n",
      "y7ycxz\n",
      "plgcpb\n",
      "bufp91\n",
      "zlobg8\n",
      "ul49ej\n",
      "skc72q\n",
      "125vn2c\n",
      "qaouds\n",
      "j01y9u\n",
      "10rx6tv\n",
      "zaqbwr\n",
      "o7c0z7\n",
      "8fzkwc\n",
      "x2lsij\n",
      "lpo2ih\n",
      "ds1xvc\n",
      "vpwqn0\n",
      "ezv3f2\n",
      "qrbkc7\n",
      "qd990q\n",
      "jk9uy4\n",
      "eak3ze\n",
      "evdtm2\n",
      "ea2gap\n",
      "ourf92\n",
      "joxdp4\n",
      "fy6tqq\n",
      "98wrkw\n",
      "732rxz\n",
      "rjzooe\n",
      "razsj2\n",
      "f7cdwg\n",
      "tzowos\n",
      "tb0jm6\n",
      "okz1j5\n",
      "jyvog1\n",
      "dc0a5f\n",
      "kykfh0\n",
      "kht9bd\n",
      "43fl90\n",
      "r8tsv6\n",
      "nl58at\n",
      "9lzabc\n",
      "6c0cc4\n",
      "rwu29s\n",
      "d9a3o1\n",
      "yav5gl\n",
      "f8wsyg\n",
      "e2jj8b\n",
      "dbgcvy\n",
      "8b4vi0\n",
      "3j295y\n",
      "uhirub\n",
      "v9of4r\n",
      "gs23ks\n",
      "fd43g9\n",
      "c3e9qu\n",
      "860311\n",
      "42ymo8\n",
      "k6467v\n",
      "d3cage\n",
      "b0rdsi\n",
      "11z3ymj\n",
      "sdzkex\n",
      "pd4jle\n",
      "br8kt3\n",
      "rga91a\n",
      "cgmptl\n",
      "iyhhgt\n",
      "qkj2yo\n",
      "lnmzv2\n",
      "fhvho3\n",
      "98ulq8\n",
      "82ed9v\n",
      "81h5c9\n",
      "szluwh\n",
      "mhh5zu\n",
      "10ivzyj\n",
      "11bwn2m\n",
      "zby4e4\n",
      "yhx3g3\n",
      "co37ut\n",
      "11yiuna\n",
      "mc77r3\n",
      "8u8ol7\n",
      "7u2xsq\n",
      "it44ix\n",
      "bpriqx\n",
      "10azzbz\n",
      "h940xb\n",
      "fbkswv\n",
      "f9obl9\n",
      "bzygxl\n",
      "ass30h\n",
      "vtd6ln\n",
      "m5miai\n",
      "kgttly\n",
      "hzq8s8\n",
      "g18xad\n",
      "e9rxcr\n",
      "8z19gw\n",
      "qwr4od\n",
      "xxmctx\n",
      "9dcltp\n",
      "xk31n8\n",
      "khin4c\n",
      "ch0qms\n",
      "558yhx\n",
      "4xgkoa\n",
      "122s5kd\n",
      "10gtruu\n",
      "hnh10y\n",
      "gb08da\n",
      "z0pw8d\n",
      "vfl57t\n",
      "pzo9e1\n",
      "og8pmh\n",
      "nw9zkt\n",
      "cnvc3e\n",
      "m4fhfg\n",
      "d8jheo\n",
      "q0n0u7\n",
      "flbqyp\n",
      "4j0u2z\n",
      "rr7cn2\n",
      "qjn0vg\n",
      "finjdz\n",
      "fefsu4\n",
      "ema1ba\n",
      "dhh2qg\n",
      "fuifr0\n",
      "kww5nf\n",
      "4hqwza\n",
      "r76igz\n",
      "l432gk\n",
      "11h3p2x\n",
      "uh5e2f\n",
      "agiatj\n",
      "11okrni\n",
      "10v75gc\n",
      "eq3da0\n",
      "10st28f\n",
      "80rhvh\n",
      "wfh1zy\n",
      "vq24py\n",
      "puz9kw\n",
      "102h06p\n",
      "rtsmm7\n",
      "r540ko\n",
      "phvgzb\n",
      "yhe96t\n",
      "x0o1nt\n",
      "ra335f\n",
      "plrrss\n",
      "rd3oby\n",
      "knai5q\n",
      "su5jia\n",
      "10k528k\n",
      "k77sxz\n",
      "hh5jy4\n",
      "b3bhwm\n",
      "snmtzn\n",
      "lof6oa\n",
      "j8gece\n",
      "a8yllj\n",
      "sx9o1g\n",
      "lgxtrm\n",
      "gulkrs\n",
      "b8pzss\n",
      "9zw14f\n",
      "vgmv86\n",
      "iby0lm\n",
      "dzkcaf\n",
      "bglwhy\n",
      "8yggag\n",
      "zgoxwa\n",
      "uq31ke\n",
      "gp98rt\n",
      "fedkoa\n",
      "de5wam\n",
      "g20x47\n",
      "ckdnkf\n",
      "11vq01a\n",
      "10pb1y3\n",
      "gm80x2\n",
      "fjr27e\n",
      "e3buo3\n",
      "bse25u\n",
      "bdviis\n",
      "8ns7vv\n",
      "u3jk05\n",
      "ayd01o\n",
      "yzw889\n",
      "sonjst\n",
      "r9hb9q\n",
      "49snc2\n",
      "11kzkla\n",
      "f3v7l5\n",
      "x2ro5v\n",
      "wrxoua\n",
      "6a97pt\n",
      "nguua9\n",
      "mqqnxj\n",
      "47zxox\n",
      "s4c6ob\n",
      "m8ewph\n",
      "lbr2fb\n",
      "fn9414\n",
      "m1vvmz\n",
      "vjkssf\n",
      "v9xme1\n",
      "tvekdw\n",
      "mzor46\n",
      "49wrt4\n",
      "11l2ojh\n",
      "rovtz1\n",
      "mku08q\n",
      "fzss9t\n",
      "86ipxh\n",
      "10kdeex\n",
      "kf86zh\n",
      "dp389c\n",
      "7muyz2\n",
      "obwojn\n",
      "lvuwf7\n",
      "jgknop\n",
      "6qvbu8\n",
      "dw0ysp\n",
      "mle2rm\n",
      "bpz0s8\n",
      "sx0e0w\n",
      "n1wnp6\n",
      "lqh9br\n",
      "cbnftu\n",
      "9h77lb\n",
      "8d388w\n",
      "wqrw8x\n",
      "lu9gen\n",
      "etdiz9\n",
      "11k84qx\n",
      "10eye8i\n",
      "w18exh\n",
      "sqra1n\n",
      "phjecd\n",
      "mdlnvy\n",
      "hbzd5o\n",
      "b7wir0\n",
      "9q11l5\n",
      "smeqbr\n",
      "pkvt4n\n",
      "o8x5uo\n",
      "kk55ww\n",
      "cdhpip\n",
      "4eila2\n",
      "117bptb\n",
      "xvhiml\n",
      "mj6i1c\n",
      "frno4g\n",
      "6jks9o\n",
      "10m6kpq\n",
      "tflvuy\n",
      "nooiha\n",
      "myurtx\n",
      "kkgyag\n",
      "k5ryva\n",
      "j5da98\n",
      "g48cu0\n",
      "ct7t1d\n",
      "wfzrdk\n",
      "wcug1f\n",
      "eb5s3l\n",
      "113m3ea\n",
      "9hrxqf\n",
      "lmbipq\n",
      "ip4lfv\n"
     ]
    }
   ],
   "source": [
    "for result_top in reddit.subreddit('MachineLearning+artificial+datascience').top(time_filter=\"all\",limit=1000):\n",
    "    print(result_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "find attribute from PRAW.READTHEDOCS.IO \n",
    "\n",
    "https://praw.readthedocs.io/en/stable/code_overview/models/submission.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_posts(subreddit_list='MachineLearning', limit=1000, time_filter='all'):\n",
    "    \n",
    "    reddit = praw.Reddit(client_id='xxx',\n",
    "                     client_secret='xxx',\n",
    "                     redirect_uri=\"xxx\",\n",
    "                     user_agent='xxx')\n",
    "    posts=reddit.subreddit(\"MachineLearning+artificial+datascience\").top(time_filter='all',limit=3000)\n",
    "    posts_df=[]\n",
    "    \n",
    "    for post in posts :\n",
    "        posts_df.append({'post_id':post.id,\n",
    "                    'subreddit':post.subreddit,\n",
    "                    'created_utc':post.created_utc,\n",
    "                    'selftext':post.selftext,\n",
    "                    'post_url':post.url,\n",
    "                    'post_title':post.title,\n",
    "                    'link_flair_text':post.link_flair_text,\n",
    "                    'score':post.score,\n",
    "                    'num_comments':post.num_comments,\n",
    "                    'upvote_ratio':post.upvote_ratio})\n",
    "    \n",
    "    return pd.DataFrame(posts_df)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>post_url</th>\n",
       "      <th>post_title</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.589117e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/v492uoheuxx41</td>\n",
       "      <td>[Project] From books to presentations in 10s w...</td>\n",
       "      <td>Project</td>\n",
       "      <td>7827</td>\n",
       "      <td>186</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kuc6tz</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.610275e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/25nxi9ojfha61</td>\n",
       "      <td>[D] A Demo from 1993 of 32-year-old Yann LeCun...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>5872</td>\n",
       "      <td>133</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g7nfvb</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.587789e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/rlmmjm1q5wu41</td>\n",
       "      <td>[R] First Order Motion Model applied to animat...</td>\n",
       "      <td>Research</td>\n",
       "      <td>4767</td>\n",
       "      <td>111</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lui92h</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.614525e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/ikd5gjlbi8k61</td>\n",
       "      <td>[N] AI can turn old photos into moving Images ...</td>\n",
       "      <td>News</td>\n",
       "      <td>4702</td>\n",
       "      <td>230</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ohxnts</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.625977e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://i.redd.it/34sgziebfia71.jpg</td>\n",
       "      <td>[D] This AI reveals how much time politicians ...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>4581</td>\n",
       "      <td>228</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id        subreddit   created_utc selftext  \\\n",
       "0  gh1dj9  MachineLearning  1.589117e+09            \n",
       "1  kuc6tz  MachineLearning  1.610275e+09            \n",
       "2  g7nfvb  MachineLearning  1.587789e+09            \n",
       "3  lui92h  MachineLearning  1.614525e+09            \n",
       "4  ohxnts  MachineLearning  1.625977e+09            \n",
       "\n",
       "                              post_url  \\\n",
       "0      https://v.redd.it/v492uoheuxx41   \n",
       "1      https://v.redd.it/25nxi9ojfha61   \n",
       "2      https://v.redd.it/rlmmjm1q5wu41   \n",
       "3      https://v.redd.it/ikd5gjlbi8k61   \n",
       "4  https://i.redd.it/34sgziebfia71.jpg   \n",
       "\n",
       "                                          post_title link_flair_text  score  \\\n",
       "0  [Project] From books to presentations in 10s w...         Project   7827   \n",
       "1  [D] A Demo from 1993 of 32-year-old Yann LeCun...      Discussion   5872   \n",
       "2  [R] First Order Motion Model applied to animat...        Research   4767   \n",
       "3  [N] AI can turn old photos into moving Images ...            News   4702   \n",
       "4  [D] This AI reveals how much time politicians ...      Discussion   4581   \n",
       "\n",
       "   num_comments  upvote_ratio  \n",
       "0           186          0.99  \n",
       "1           133          0.98  \n",
       "2           111          0.97  \n",
       "3           230          0.97  \n",
       "4           228          0.96  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    posts_df=get_top_posts(subreddit_list=\"MachineLearning+artificial+datascience\", limit=3000,time_filter='all')\n",
    "    posts_df.to_csv('Reddit_DS_ML_AI_posts.csv',header=True, index=False)\n",
    "\n",
    "    posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.987000e+03</td>\n",
       "      <td>2987.000000</td>\n",
       "      <td>2987.000000</td>\n",
       "      <td>2987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.603668e+09</td>\n",
       "      <td>485.015400</td>\n",
       "      <td>74.031135</td>\n",
       "      <td>0.950673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.571406e+07</td>\n",
       "      <td>521.102178</td>\n",
       "      <td>93.023462</td>\n",
       "      <td>0.039120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.365941e+09</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.569607e+09</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.610517e+09</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.650934e+09</td>\n",
       "      <td>546.500000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.680682e+09</td>\n",
       "      <td>7827.000000</td>\n",
       "      <td>2361.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc        score  num_comments  upvote_ratio\n",
       "count  2.987000e+03  2987.000000   2987.000000   2987.000000\n",
       "mean   1.603668e+09   485.015400     74.031135      0.950673\n",
       "std    5.571406e+07   521.102178     93.023462      0.039120\n",
       "min    1.365941e+09    80.000000      0.000000      0.660000\n",
       "25%    1.569607e+09   189.000000     19.000000      0.940000\n",
       "50%    1.610517e+09   364.000000     46.000000      0.960000\n",
       "75%    1.650934e+09   546.500000    102.000000      0.980000\n",
       "max    1.680682e+09  7827.000000   2361.000000      1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieve the comment from the user id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter thread: [https://twitter.com/cyrildiagne/status/1259441154606669824](https://twitter.com/cyrildiagne/status/1259441154606669824)\n",
      "\n",
      "Code: [https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard](https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard)\n",
      "\n",
      "Background removal is done with U^(2-Net) (Qin et Al, Pattern Recognition 2020): [https://github.com/NathanUA/U-2-Net](https://github.com/NathanUA/U-2-Net)\n",
      "\n",
      "**/!\\\\ EDIT:** You can now subscribe to a beta program to get early access to the app: [https://arcopypaste.app](https://arcopypaste.app)  !\n",
      "The future ðŸ¤¯\n",
      "Simple yet very useful. Thank you for sharing the code.\n",
      "Almost guaranteed, Apple will copy your idea in 3, 2, 1....\n",
      "Ohh the nightmare of making this into a stable product... Enough to drive you mad just thinking about it\n",
      "Wtffff. Well that was incredible.\n",
      "Apple canâ€™t wait to steal this and not credit the creators\n",
      "fantastic!\n",
      "Why did the boxes in the diagram turn gray?\n",
      "How does the Algorithm decide what it cuts out from the input pictures? \n",
      "\n",
      "For example it only cut out the two people in the picture and not the surroundings.\n",
      "\n",
      "Amazing project though!\n",
      "#WITCH!  BURN THEM!\n",
      "Any sufficiently advanced technology is indistinguishable from magic.\n",
      "This will be amazing if released, even as a beta. Definitely can see this being very useful\n",
      "Really good work, thanks for sharing!\n",
      "I'm extremely impressed with it cutting dark hair from a brown background. Is that the pixel's camera doing the hard work or is it U^2_Net ? Have you tried it with other phones? How does it deal with feathering? Stunning demo & thanks for posting this.\n",
      "Super cool\n",
      "Wizardry!\n",
      "Woahhh that is so cool!!! I am wondering the speed wise from the initial snap till pasting it to computer.\n",
      "\n",
      "If we could get it done >1s I think this project would be really fun and useful. Allow me to fork the project ;)\n",
      "\n",
      "Thank youuu\n",
      "This is God like!\n",
      "Wow. What you did wlth AR is really creative and very impressive technically. Keep going dude you rock.\n",
      "Holy fucking shit my jaw hasnâ€™t dropped like this since I saw the GPT-2 demo. This is absolutely unrealâ€”it is so precise + how the hell do they interact with macOS like that? Wow. Awesome work pal, so much respect.\n",
      "Super cool demo.\n",
      "\n",
      "But the more interesting part to me is the app actually look at the computer screen to decide what target the image/content is pasted to. \n",
      "\n",
      "Probably hard-coded, but super interesting idea.\n",
      "This is amazing. Congratulations!!!\n",
      "This is amazing! Thanks for sharing the code\n",
      "Awesome! Recognize the catalog from Coder le Monde\n",
      "That is so cool!\n",
      "Awesome, will try it definitely.\n",
      "Take my money\n",
      "God this, and swiping a window to my laptop from my phone with a simple gesture, is what I have been waiting for sooo long.\n",
      "cyberpunk level shit\n",
      "Wow. Thanks for sharing!\n",
      "This is really well done. From research to a simple yet useful use case!\n",
      "beautiful\n",
      "This is brilliant. Thanks for sharing...\n",
      "Say sike ðŸ¤¯ðŸ¤¯\n",
      "I saw this the other day and I thought it was incredible. I'm a novice on programming but ill do my best to deploy this on my PC just to play around with it! Thanks a lot for sharing this with the world!\n",
      "Smart move\n",
      "That's some next level copy -paste !\n",
      "This is so cool! AI never ceases to amaze me.\n",
      "10 years ago people would laugh at this idea.\n",
      "Wow this is so helpful, insane\n",
      "This is so crazy!\n",
      "So good it looks fake af\n",
      "This is probably the coolest thing I have seen in a long while.  Great fucking work!\n",
      "Wow, this is sick\n",
      "You sir are a genius\n",
      "What are the edges cases when this doesn't work? Does this require certain lighting conditions etc? How does it know to extract both people from the image?\n",
      "Very impressive, thought it was fake at first.... ðŸ¤ª\n",
      "Amazing!\n",
      "Wow.\n",
      "This gets 100 very nices\n",
      "This is insane.\n",
      "Man, this is awesome!\n",
      "This is amazing. If you have any intention of publishing this as an end user app, hit me up, Iâ€™ll get make sure you get sponsorship for all the GPUs and other compute you need.\n",
      "This is brilliant!\n",
      "This is some crazy Tony stark shit\n",
      "This is something really superb!!!!!!!!!!!  \n",
      "I loved the technology...\n",
      "\n",
      "AI and Machine learnings are actually contributing a lot in streamlining our daily processes. I mean, this is something, being a student I would need the most, instead of first emailing myself pictures from phone, then downloading them and inserting them in my doc.\n",
      "Woke up in the morning and this is the first thing I see. A day canâ€™t get more inspirational.  I canâ€™t thank you enough for sharing.\n",
      "Wow, this is epic!\n",
      "What is difference between this and taking photo and sending it with email to computer? ðŸ¤” What is the main use case for this technology?\n",
      "I really hope your idea doesn't get stolen. Also how do I keep up to date with your progress?\n",
      "Did you train the ML model yourself? If so what data set did you use?\n",
      "How were you able to get integration with chrome and slides itself? Are you able to load custom software through Google Slides somehow?\n",
      "How do I do this?\n",
      "This is so cool! Is it really necessary to point the phone at the screen to paste it?  Or will it just paste it into whatever application is currently focused no matter what?\n",
      "[deleted]\n",
      "u/fabiomb el otro dÃ­a decÃ­as que andaba porque tenÃ­a fondo de color blanco plano.\n",
      "I'm more impressed with the background extraction on the photo than with the multidevice \"copy-paste\"\n",
      "u/vRedditDownloader\n",
      "u/VeedditDownloader\n",
      "u/vredditdownloader\n",
      "That's so cool\n",
      "There's no way that took 10s to develop, install, try and record an 57 sec video of. I mean, yeah, technology and stuff, but not in 10s. Sorry.\n",
      "I will go through the damn code line by line!\n",
      "Is that a Google Pixel?\n",
      "I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:\n",
      "\n",
      "- [/r/mattslinks] [Augmented reality cut n paste](https://www.reddit.com/r/mattslinks/comments/ht7vs5/augmented_reality_cut_n_paste/)\n",
      "\n",
      "&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*\n",
      "This is beast!! Deff on to something!\n",
      "Awesome.\n",
      "How do we use this\n",
      "more more..More..MOREEEEEE\n",
      "Dude...love ur copy paste...\n",
      "WITCH!\n",
      "What the fuuuuuuuuck?\n",
      "photoshop required ?\n",
      "Omg! Thats awesome!!! I had a similar idea but using text\n",
      "u/vredditdownloader\n",
      "This is beyond science\n",
      "My 5 years daughter thought of something similar..for her she wants that you take the object out of the screen and you show its hologram presentation..she said that would be a hard project to achieve :))\n",
      "I will show her your project tomorrow, she will like it.\n",
      "Oh sure. I find this after spending 29 days scanning in 21 years of issues of an instructional magazine on a flat bed scanner\n",
      "[deleted]\n",
      "Really impressive if it works as well with unseen data.\n",
      "\n",
      "Still fun if it doesn't.\n",
      "This is clearly fake....the last screen shot proves it.\n",
      "In 3 seconds if you use anything else then MacBook ðŸ˜\n",
      "Ok, thatâ€™s the coolest thing Iâ€™ve seen in a long while.\n",
      "This is awesome.\n",
      "Tony Stark shit\n",
      "Good job\n",
      "Let we know when there's an easy and seamless way of doing this, or at least no-brainer\n",
      "Wow dude, I don't know shit about ML, all I can say is this is superpower\n",
      "I need this for editing for my small business. No more Adobe illustrator.\n",
      "fuck capitalism\n",
      "\n",
      "\n",
      "wreak havoc on the middle class\n",
      "Youâ€™re going to be rich\n",
      "If it wasn't for the link with the code, I would have straight up thought you were trying to trick us :o\n",
      "This is insaneeee! I have a question about it's applications:\n",
      "\n",
      "is it possible to use this to extract information? For example if you scan a receipt could it create a digital version with each line editable?\n",
      "Holy smokes this is insanely awesome. Thank you\n",
      "I mean, technically now it's the past.\n",
      "Took words out of my mouth\n",
      "His license even allows commercial use, so they are legally allowed to do that\n",
      "Lol and he's using a pixel too\n",
      "Likely, and it will be easier for them, the processing could be done in the iPhone and uploading can be done through airdrop (which supports 'aiming' at people and machines to share files).\n",
      "Why would it be a nightmare?\n",
      "[deleted]\n",
      "They'll probably slap a patent on it too and sue the original creators.\n",
      "> Apple canâ€™t wait to steal this and not ~~credit~~ **sue** the creators\n",
      "\n",
      "ftfy\n",
      "Already stolen and and implemented into the next iOS...\n",
      "Thanks!\n",
      "U^2-Net decided not to remove the background of these :)\n",
      "Check out the details for U^2-Net on the official repo: https://github.com/NathanUA/U-2-Net\n",
      "and any sufficiently understood magic is indistinguishable from technology.\n",
      ">, even as a beta\n",
      "\n",
      "The code is available, so you can play already with it :)   \n",
      "[https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard](https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard)\n",
      "It is 100% handled by U^2-Net: Check out the official repo for more information and samples: https://github.com/NathanUA/U-2-Net\n",
      "Yep maybe add a ghost non-transparent-background\n",
      "to make up for the delay of the BG removal.\n",
      "\n",
      "I'm just impressed by the copy paste AR stuff. Well done!\n",
      "Hi! The coordinates are automatically defined by the receiving software in this demo but checkout my precious demo where I use OpenCV SIFT to find the correct coordinates on the screen\n",
      "Please checkout the official U^2-Net repo for more information on the background substraction: https://github.com/NathanUA/U-2-Net\n",
      "Edge cases mostly are busy scenes when there are no particular salient element\n",
      "It just save time and headaches but the result is identical\n",
      "\n",
      "Although you get background removal for free in the process ;)\n",
      "Looks like it also recognizes the photo subject to only copy the link important bits. Also faster\n",
      "Thanks! For now the most updated news are on my Twitter!\n",
      "I'm using the pretrained model from U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection, Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R. Zaiane and Martin Jagersand: https://github.com/NathanUA/U-2-Net\n",
      "Juste the clipboard and pyautogui to send the \"paste\" keystrokes :)\n",
      "Checkout the repository!\n",
      "Good point! For now the code only paste at whatever app is active. In some apps (like Photoshop) you can paste at specific coordinates depending on where you point the phone\n",
      "Not yet ;)\n",
      "AhÃ­ me gustÃ³ mÃ¡s ðŸ‘\n",
      "*beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!\n",
      "\n",
      "* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)\n",
      "\n",
      "* [Audio only](https://v.redd.it/v492uoheuxx41/audio)\n",
      "\n",
      "I also work with links sent by PM\n",
      "\n",
      " ***  \n",
      "[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;â¤**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader)\n",
      "*beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!\n",
      "\n",
      "* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)\n",
      "\n",
      "* [Audio only](https://v.redd.it/v492uoheuxx41/audio)\n",
      "\n",
      "I also work with links sent by PM\n",
      "\n",
      " ***  \n",
      "[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;â¤**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader)\n",
      "have you had a chance to do that? has anyone tried running that code on their device?\n",
      "Yes! But it also work with iphones\n",
      "*beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!\n",
      "\n",
      "* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)\n",
      "\n",
      "* [Audio only](https://v.redd.it/v492uoheuxx41/audio)\n",
      "\n",
      "I also work with links sent by PM\n",
      "\n",
      " ***  \n",
      "[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;â¤**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader)\n",
      "Hmm, I don't think the 2nd image with the 2 persons could have be done with OpenCV?\n",
      "What do you mean? The service runs remotely and it has never seen the images used in the video\n",
      "I guarantee it's not fake.\n",
      "Is the ELI5 version is this? - \n",
      "\n",
      "1. The React Native application on the mobile takes a pictures\n",
      "2. The picture get uploaded to the server where the background is removed with u2-net, which is the brains of the operation\n",
      "3. The removed background is then communicated through a python script > Photoshop plugin to be added on to a running project or is saved as a transparent image.\n",
      "\n",
      "Right?\n",
      "Yes. But you will need to a run OCR instead of the background removing application (U\\^2-Net)  OP is using.\n",
      "This would be the killer use case.\n",
      "How can it be the past if its happening now?... id say the now its the future and past overlapping for a jiffy of a second\n",
      "did you sue him\n",
      "[deleted]\n",
      "Well, why don't we already have cross-device AR interfaces and can swipe content between our devices seamlessly like Tony Stark? The U-net demonstrated here only provides a means to extract relevant sections of image data. The rest that needs to be done for this demo is far more difficult. Roughly, you need to identify the other device through the camera or NFC, pinpoint the relative position of the two devices for the onscreen insertion position, match the other device to a Bluetooth device or wifi connected device securely, set up a transfer, communicat data type and decide what should happen with the data... and do all of this across different OS and devices with different standards, handle poor connection, communicate all the issues to the user in a foolproof way. You can force the user to setup some of this manually, but then you'll loose 99% of the users and the product won't gain enough support/funding and gets dropped like a pair of Google glasses.\n",
      "Wait. I thought they'll sew my ass to the mouth of another person who accepted the ToS.\n",
      "But capitalism drives innovation by rewarding innovators. That's why we have all the smart humanitarian millionaires pushing humanity towards brighter future.\n",
      "\n",
      "/s\n",
      "I put up a public predictor API endpoint for the ML model so you don't have to battle with GPUs when playing with this. Simply start the server with `--basnet_service_ip http://basnet-predictor.tenant-compass.global.coreweave.com/` and that piece is taken care off.\n",
      "Beautiful thanks! I'm excited to try this.\n",
      "How long does inference take generally? Is your video realtime? Because it's surpringly fast for an HD photo from a phone.\n",
      "How do you handle the domain shift between digital images and photos of images captured with a camera?\n",
      "\n",
      "(i.e. perspective, glare, curvature, lighting)  \n",
      "Or do you just hope the pretrained network generalizes well enough?\n",
      "I see what you are talking about. Yess, definitely can do that!! OP is a badass\n",
      "If it removes the background you want.\n",
      "Oh...true. I was overthinking it haha\n",
      "RemindMe! 4 days\n",
      "Cool! I always assume the examples used in presentations are part of the training data unless told otherwise.\n",
      "\n",
      "~~From a quick look at the code, I guess it's based on this paper?~~  [~~http://openaccess.thecvf.com/content\\_CVPR\\_2019/papers/Qin\\_BASNet\\_Boundary-Aware\\_Salient\\_Object\\_Detection\\_CVPR\\_2019\\_paper.pdf~~](http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Nevermind, the description on Github answers that question, was just to lazy to read it before jumping into the code :P\n",
      "Awesome! thanks for the reply! \n",
      "\n",
      "Completely understand if you're not able offer this, but do you know of any good, reliable OCR services?\n",
      "\n",
      "All the ones I've found, tend to be geared towards a particular functionality and after testing several they just don't live up to the standard I'm expecting\n",
      "No it wouldn't, because it already exists in dozens of apps. Including Google Lens, installed on hundreds of millions of phones.\n",
      "Future, past, now is simply an illusion. Reality is one continuum, it can't be neatly divided into division as such. But, this kind of conceptualization might have tremendous utility in our daily life.\n",
      "Why bother patenting it? Does he have the money to patent it, does he have the money to protect the patent against apple or google, can it be patented, does he have the intellectual resources to patent...\n",
      "\n",
      "Patents are for companies to monetise their R&D, not for individuals to get rich. This dude would probably be happy enough getting some corporate credibility and could potentially lead a team if google or apple are interested in this.\n",
      "Yeah ok, we'll wait here to hear the news in 3 years about whether or not it got approved.\n",
      "Yeah man and this all those companies fault who use different standard for every fucking thing (microsoft and apple looking at u) .\n",
      "You just need to run some visualbasic to check the camera on the phone to know where on the pc it's pointing to, then send that image to the pc with the coords to paste the amethyst. I can do this with one weekend.\n",
      "What system does drive innovation then? Do you wanna say that socialism/communism pushed their country towards brighter future?\n",
      "Video is real-time but inference is don't on a 320x320 image. But that's only the resolution of the alpha mask , the image can have native reslution\n",
      "I will be messaging you in 4 days on [**2020-05-15 12:56:47 UTC**](http://www.wolframalpha.com/input/?i=2020-05-15%2012:56:47%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/gh1dj9/project_from_books_to_presentations_in_10s_with/fq9lu04/?context=3)\n",
      "\n",
      "[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Fgh1dj9%2Fproject_from_books_to_presentations_in_10s_with%2Ffq9lu04%2F%5D%0A%0ARemindMe%21%202020-05-15%2012%3A56%3A47%20UTC) to send a PM to also be reminded and to reduce spam.\n",
      "\n",
      "^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20gh1dj9)\n",
      "\n",
      "*****\n",
      "\n",
      "|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n",
      "|-|-|-|-|\n",
      "Have you tried tesseract? One year back i tried it with a little project, it work quite well out of the box.\n",
      "[deleted]\n",
      "Where can I read more about this ?\n",
      "https://xkcd.com/927/\n",
      "Sure! Go for it buddy.\n",
      "I think if workers were to own their workplaces, instead of stock holders, we would be living in better world. In effect that means you can only own a piece of company if you work there.\n",
      "\n",
      "This is theory, I don't think this has ever been tried in any existing society. There are some worker-cooperatives but they have to compete with capitalist companies which do not have the moral restrictions a worker owned company have, so they are naturally at a disadvantaged position.\n",
      "no never heard of it! kicking myself now that I wasn't able to find it whilst looking for a solution.\n",
      "\n",
      "This looks promising so thank you :) I'm a designer with some programming knowledge myself so I imagine it'll take a while to really test it out for my purpose.\n",
      "\n",
      "Thanks so much again!\n",
      "Makes sense!\n",
      "Idk I cooked that up [I'm a wannabe philosopher ;)] but you can find many parallels in some Philosophers' works like Neitzsche's Cause and effect theory.\n",
      "\n",
      "I must add, both Space and Time are wholly inference based derivatives, we can't or haven't perceived them. All the Space-time continuum metaphysical talks are purely theoretical. No experiment have given emperical evidence of them.\n",
      "Wow never thought like that cheers !!!\n",
      "dId He dO It\n",
      "So worker-owned companies have been tried and failed, but it's capitalism's fault?\n",
      "\n",
      "Actually, how do these things even work? Who makes the decisions in the absence of clear owner? What constitutes a \"worker\"? How are the company shares divided between workers - evenly, or according to their positions?\n",
      "Just to be clear: Apple is a company that was owned by its workers initially. Those workers decided to sell a piece of it to investors early on because they (the workers) decided it would help them grow faster. Then they were successful, and they (the workers) decided to sell more of the company so they could buy nice houses and make charitable contributions. Apple *is* the result of a worker-owned-company system, albeit one that gave those workers the freedom to sell their stock for various reasons along the way.\n",
      "Thank you. Sounds fascinating!\n",
      ">So worker-owned companies have been tried and failed, but it's capitalism's fault?\n",
      "\n",
      "That's like saying \"not stealing has been tried but it was less profitable to stealing, so now you are blaming thieves that the losers who don't steal lost?\"\n",
      "\n",
      ">Actually, how do these things even work? \n",
      "\n",
      "You can research these topics yourself. For example /r/Anarchy101 has smarter people than me to explain these topics.\n",
      "That's interesting. Didn't know that. But Apple was worker-owned company until it turned into capitalist company. When they hired their first employee who didn't own a piece of that company, they made an ideological choice to exclude that employee from the profits the company produced and in effect created hierarchy inside their company.\n",
      ">\"not stealing has been tried but it was less profitable to stealing, so now you are blaming thieves that the losers who don't steal lost?\"\n",
      "\n",
      "The reason not to steal is because there are deliberate mechanisms in place to dissuade, not because it's a less efficient way to make money. In contrast, socialism *is* less efficient than capitalism.\n",
      "\n",
      ">You can research these topics yourself.\n",
      "\n",
      "Why is it that every single anti-capitalist assigns homework when poked with a stick? Consider that you are arguing in favor of uprooting industry as a whole and can't even articulate why.\n",
      "\n",
      ">This is theory, I don't think this has ever been tried in any existing society.\n",
      "\n",
      "This is another favorite. You are betting the farm -- hell, *society* -- on a theory. On something that has not even been validated. Doesn't that seem a little bonkers to you?\n",
      "Actually, in most countries stealing is less profitable than any legitimate way of earning money thanks to law enforcement. So in the same vein, it's USA government's fault for letting capitalism go unchecked there, but no fault of the system itself.\n",
      "I suspect for much of Appleâ€™s life, the vast majority of Apple employees owned a piece of the company. I donâ€™t know if Apple Store employees do - they may not - but Apple stores are a comparatively recent addition and I bet the engineering staff through the 90s were employee-owners as thatâ€™s the norm in Silicon Valley. At some point Apple made a decision to contract out manufacturing, so the people actually building Apple computers and phones are mostly not Apple employees and not owners. And it was certainly a significant decision to bring on venture capital investors (and later public shareholders) who were not employees of the company (though I think history would show pretty clearly that if they hadnâ€™t done that we wouldnâ€™t have Apple today). \n",
      "\n",
      "But I do think itâ€™s worth noting that all of these *decisions* were, at Apple, mostly made by the early employees / founders, not by third party shareholders. Thatâ€™s idiosyncratic to tech, an industry dominated by strong founders, but itâ€™s true at Apple, at Google, at Facebook, at Amazon, etc - the decision makers are the founding employees. In fact at many of these companies the founders have put in place systems such that â€œcapitalistâ€ public owners explicitly DONâ€™T have control of the business, only the founders do, long after the founder ownership levels have decreased.\n",
      "\n",
      "Even Goldman Sachs was a â€œpartnershipâ€ for most of its history - ENTIRELY owned by a subset of its employee population. This is true for every major large corporate law firm today. Just because these businesses are â€œemployee ownedâ€ clearly doesnâ€™t mean theyâ€™re run â€œfor the benefit of the peopleâ€ - theyâ€™re run by rich early employees who want to get richer (and maybe have other motivations, like building great products, or personal celebrity, or whatever). \n",
      "\n",
      "I am a huge fan of â€œemployee equity ownershipâ€ - most startups are built on the back of this idea - though most employees in turn eventually want to be able to sell their shares to other people (so they can buy houses and cars, or make charitable contributions or whatever). But Iâ€™m not sure employee ownership is a radical departure from â€˜capitalismâ€™ as you describe it - the ends ultimately look pretty similar to companies that are not employee owned.\n",
      "> Why is it that every single anti-capitalist assigns homework when poked with a stick? \n",
      "\n",
      "Because I'm not your teacher. You look up this stuff yourself if you are interested.\n",
      "\n",
      ">You are betting the farm -- hell, society -- on a theory. On something that has not even been validated. Doesn't that seem a little bonkers to you?\n",
      "\n",
      "That's the dilemma of sociology in general. You can't run controlled experiments without affecting human lives and you can't remove yourself from the equation and be a neutral observer because you are part of that society.\n",
      "> Because I'm not your teacher. You look up this stuff yourself if you are interested.\n",
      "\n",
      "I am interested, and I have tried to look it up. I'm pointing out that it's not a compelling argument to say \"go look it up\" when you are trying to change the status quo.\n",
      "\n",
      "It doesn't matter whether you convince me or not; I'm not here to be convinced, and you aren't here to convince me. The problem is that *no one* can seem to articulate why we should be socialist. It always ends up as a homework assignment no matter who I talk to. \n",
      "\n",
      ">That's the dilemma of sociology in general.\n",
      "\n",
      "I agree. \n",
      "\n",
      "Fortunately for capitalists (and unfortunately for you), capitalism isn't the outcome of a controlled experiment. Rather, it is [*emergent*](https://en.wikipedia.org/wiki/Emergence). The free market exists as a result of every individual acting according to his or her individual incentives, not because a committee decided that this is the way we should do it.\n",
      "\n",
      "So not only is the statement that we should uproot the entire economic system incredibly arrogant, since it is predicated on the assumption that you will implement it properly (in the context of the incentives that exist for the people implementing it), proving that it will work at all requires evidence that cannot be obtained. As you noted, there is no way to conduct a controlled experiment in this area, so we are kind of stuck with what we've got.\n",
      "\n",
      "[Here](https://en.wikipedia.org/wiki/Holodomor) is what happens when you get too clever.\n"
     ]
    }
   ],
   "source": [
    "submission=reddit.submission(\"gh1dj9\")\n",
    "\n",
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comments_list=[]\n",
    "\n",
    "# for post_id in posts_df['post_id']:\n",
    "#    submission=reddit.submission(post_id)\n",
    "#    submission.comments.replace_more(limit=None)\n",
    "# for comment in submission.comments.list():\n",
    "#    comments_list.append({'post_id':post_id,'comment':comment.body})\n",
    "    \n",
    "### create a dataframe\n",
    "\n",
    "# comments_df=pd.DataFrame(comments_list)\n",
    "# comments_df.to_csv('DS_ML_AI_COMMENTS.CSV',header=True, index=False)\n",
    "# comments_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code appears to collect comments from Reddit submissions and store them in a pandas DataFrame called comments_df, which is then saved as a CSV file.\n",
    "\n",
    "However, there is an issue with the code. The for loop that collects comments only stores the comments from the last submission in submission.comments.list(), not all submissions. To fix this, the for loop should be indented so that it includes both the submission retrieval and comment collection steps, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226368, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_list=[]\n",
    "\n",
    "for post_id in posts_df['post_id']:\n",
    "    submission=reddit.submission(post_id)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        comments_list.append({'post_id':post_id,'comment':comment.body})\n",
    "    \n",
    "### create a dataframe\n",
    "\n",
    "comments_df1=pd.DataFrame(comments_list)\n",
    "comments_df1.to_csv('DS_ML_AI_COMMENTS1.CSV',header=True, index=False)\n",
    "comments_df1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
